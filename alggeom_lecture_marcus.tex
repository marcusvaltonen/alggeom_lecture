% !TEX encoding = UTF-8 Unicode

\documentclass[11pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[sc]{mathpazo}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage{geometry}
\usepackage[pdftex]{graphicx}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{epstopdf}

\usepackage[font=footnotesize,labelfont=bf]{caption}
\usepackage[font=footnotesize,labelfont=bf]{subcaption}
\usepackage{hyphenat}
\usepackage[inline]{enumitem}

\usepackage{mathrsfs}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

% vast and Vast brackets (larger than Huge)
\makeatletter
\newcommand{\HUGE}{\bBigg@{3}}
\newcommand{\vast}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{5}}
\makeatother
% Math in header should be boldface
\makeatletter
\g@addto@macro\bfseries{\boldmath}
\makeatother

% My own commands
\usepackage{gensymb} % degree sign
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\phi}{\varphi}

\newcommand{\fr}[2]{\frac{#1}{#2}}
\newcommand{\pf}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\cd}{\cdot}
% \newcommand{\T}{\!\mathsf{T}}
\newcommand{\T}{T}
\newcommand{\te}[1]{\text{#1}}
\newcommand{\id}{\mathrm{d}}
\usepackage{bm}
\newcommand{\mat}[1]{\bm{#1}}
\newcommand{\nullspace}[1]{\mathcal{N}{#1}}
\newcommand{\lagrange}[1]{\mathcal{L}{#1}}
\usepackage{mathtools}

\usepackage{bbm}
\newcommand{\N}{\ensuremath{\mathbbm{N}}}
\newcommand{\Z}{\ensuremath{\mathbbm{Z}}}
\newcommand{\Q}{\ensuremath{\mathbbm{Q}}}
\newcommand{\R}{\ensuremath{\mathbbm{R}}}
\newcommand{\K}{\ensuremath{\mathbbm{K}}}
\newcommand{\C}{\ensuremath{\mathbbm{C}}}

\newcommand{\lr}{\ensuremath{\mathrm{\quad\Leftrightarrow\quad}}}
\newcommand{\ra}{\ensuremath{\mathrm{\quad\Rightarrow\quad}}}
\newcommand{\lra}{\ensuremath{\mathrm{\longrightarrow}}}
\newcommand{\f}[3]{#1\,:\,#2\,\lra\,#3}
\newcommand{\norm}[1]{\left\|#1\right\|}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\atan}{atan2}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator{\nulldim}{nulldim}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\lt}{LT}
\DeclareMathOperator{\sat}{Sat}
\newcommand{\Rop}[1]{\mathcal{R}_{#1}}

\makeatletter
\DeclareRobustCommand\eg{\emph{e.g}\@ifnextchar.{}{.\@}}
\DeclareRobustCommand\etal{\emph{et~al}\@ifnextchar.{}{.\@}}
\DeclareRobustCommand\ie{\emph{i.e}\@ifnextchar.{}{.\@}}
\DeclareRobustCommand\cf{\emph{cf}\@ifnextchar.{}{.\@}}
\DeclareRobustCommand\NB{\emph{N.B}\@ifnextchar.{}{.\@}}
\DeclareRobustCommand\wrt{w.r.t\@ifnextchar.{}{.\@}}
\makeatother

\usepackage{hyperref}
\usepackage[capitalise]{cleveref}


%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\title{
\normalfont \normalsize
\textsc{Lund University --- Algebraic Geometry} \\ [7pt]
\Large Practical aspects of solving a system of polynomial equations \\
}
\author{Marcus Valtonen \"{O}rnhag} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

\section{Introduction}
In this lecture, we will discuss how to optimize the computations, both for speed and
accuracy, given a polynomial system of equations with generic coefficients. Such problems are very
frequent in computer vision, and many of the examples will be from this field of research.
I will mostly rely on examples; however,
the necessary theory is briefly discussed in this document, which you are encourage to read before
the lecture. The theory is not meant to be complete, and I will refer you to other sources for
each topic, and try to list actual publications using the particular method.

For the practical part we will use the
automatic solver by Larsson~\etal{}, which you should install and get accustomed to prior
to the lecture. The solver is available at~\url{http://people.inf.ethz.ch/vlarsson/misc/autogen_v0_5.zip}, and requires Macaulay2 to be installed. If you want to use C++ (which greatly decreases the computation time) you will need to install the Eigen library as well.\footnote{There is also a wrapped version by Bo Li~\url{https://github.com/prclibo/gaps}, that you might prefer.}
Apart from the course literature, you are also referred to Viktor Larsson's PhD
thesis for a deeper understanding of the content in this lecture.

\section{Theory}
\subsection{Recapitulation of the course contents}
Using multi-index notation,
let~$\mat{x^{\alpha}}$ denote a \emph{monomial} of degree $|\mat{\alpha}|$.
Our main goal is to solve a polynomial system of equations
\begin{equation}\label{kappa:eq:polsys}
    \begin{aligned}
    f_1(\mat{x}) & = 0, \\
                 & \hspace{0.575em} \vdots \\
    f_s(\mat{x}) & = 0,
    \end{aligned}
\end{equation}
where each polynomial equation can be expressed
as~$f=\sum_{\mat{\alpha}}c_{\mat{\alpha}}\mat{x^\alpha}$.
The set of all solutions to~\eqref{kappa:eq:polsys} is called an \emph{affine variety}, and is denoted
$\mat{V}(f_1,\ldots,f_s)\subset\C$.
Let~$\C[\mat{x}]$ denote the set
of polynomials in~$\mat{x}$ with coefficients in $\C$.
An \emph{ideal} $I\subset\C[\mat{x}]$ is an additive group satisfying the \emph{absorption property},~\ie{}~if~$f\in I$
and $h\in\C[\mat{x}]$ then $hf\in I$. Moreover, given a set of polynomials $f_1,\ldots,f_s\in\C[\mat{x}]$
we consider the ideal
\begin{equation}
\langle f_1,\ldots,f_s\rangle = \left\{\sum_{i=1}^sh_if_i\;:\;h_1,\ldots,h_s\in\C[\mat{x}]\right\},
\end{equation}
which we will refer to as the \emph{ideal generated by} $f_1,\ldots,f_s$.
The \emph{Hilbert Basis Theorem} states that every ideal of $\C[\mat{x}]$ is
finitely generated, \ie{} given an ideal~$I\subset\C[\mat{x}]$ there exist
$f_1,\ldots,f_s\in\C[\mat{x}]$ such that $I=\langle f_1,\ldots,f_s\rangle$.
Thus, the polynomial system of equations~\eqref{kappa:eq:polsys} is defined by the
generated ideal; however, the basis is in general not unique.

To be able to represent the system of equations uniquely, one must impose
a \emph{monomial ordering}, \eg{} the lexicographic order,
the graded lex order or the graded reverse lex order. Regardless of which monomial ordering
is chosen the \emph{leading term} (\wrt{}~the monomial ordering) is uniquely defined, and we shall
denote it $\lt(f)$. Furthermore, for an ideal $I$ define $\lt(I)\coloneqq\{\lt(f)\;:\;f\in I\}$. Then
a finite subset \mbox{$\mat{G}=\{g_1,\ldots,g_t\}\subset I$} is a \emph{Gröbner basis}
if $\langle\lt(g_1),\ldots,\lt(g_t)\rangle=\lt(I)$.

After this exposition one may ask: how does this relate to solving polynomial systems
of equations? The answer lies in the fact that we have efficient ways of computing
a Gröbner basis, by means of \emph{Buchberger's algorithm}.
For more details regarding the algorithm, and to get a deeper understanding of the subject,
the work of Cox~\etal{}~\cite{cox,cox2} is highly recommended.

\subsection{The action matrix method}\label{kappa:sec:actionmatrix}
Let us return to the polynomial system of equations~\eqref{kappa:eq:polsys}. Under the
assumption that the system has finitely many solutions, \ie{} when $\mat{V}(I)$ is finite,
it follows by the Finiteness Theorem (see \eg{}~\cite{cox2}) that $I$ is
zero-dimensional and the quotient space~\mbox{$A=\C[\mat{x}]/I$} is
finite dimensional. Given the \emph{coset}~$[f] = \{f+h\;:\;h\in I\}$,
consider the operator~\mbox{$T_f\::\:A\longrightarrow A$}, defined by~$T_f([g])=[fg]$.
It is easily seen that~$T_f$ is
linear with the property that~$T_f=T_g$ if and only if~$f-g\in I$.
Since the quotient space~$A$ is finite-dimensional this operation can be
represented by a matrix~$\mat{M}_f$, which is known as the \emph{action matrix}.
Furthermore, we may select a basis for~$A$,~\eg{} a monomial
basis~$\mathcal{B}=\{[\mat{x}^{\alpha_j}]\}_{j\in J}$,
typically obtained by (an improved version of)
Buchberger's algorithm.
When the action matrix~$\mat{M}_f=(m_{ij})$ acts on the basis elements we obtain a
linear combination of the monomials forming the basis, namely
\begin{equation}
    T_f([\mat{x}^{\alpha_j}]) = [f\mat{x}^{\alpha_j}]=\sum_{i\in J}m_{ij}[\mat{x}^{\alpha_i}]\;.
\end{equation}
This implies that,
% for some $h\in I$
% \begin{equation}
%     f\mat{x}^{\alpha_j}=\sum_{i\in J}m_{ij}\mat{x}^{\alpha_i},
% \end{equation}
% and
for $\mat{x}\in V(I)$,
\begin{equation}\label{kappa:eq:action}
f(\mat{x})\mat{x}^{\alpha_j}=\sum_{i\in J}m_{ij}\mat{x}^{\alpha_i}\;.
\end{equation}
By representing the basis~$\mathcal{B}$ with a vector $\mat{b}$, and using the fact
that~\eqref{kappa:eq:action} must hold for all basis elements, the problem can be reduced to
\begin{equation}
f(\mat{x})\mat{b}(\mat{x}) = \mat{M}_f^{\T}\mat{b}(\mat{x}),
\end{equation}
which we recognise as an eigenvalue problem.
Let us recapitulate: given a polynomial
system of equations, use Buchberger's algorithm to obtain a Gröbner basis. Create the
action matrix and compute the eigenvalues and eigenvectors to extract the solutions.

\subsection{Elimination template}
In practice we do not compute a Gröbner basis everytime we want to solve a polynomial system
of equations. There are many reasons for this:
\begin{enumerate*}[label={\roman*}]
\item it is computationally expensive,
\item due to floating point arithmetic, accumulated round-off errors may reduce performance,
\item degeneracies for certain instances with real data.
\end{enumerate*}

Instead we try to find the action matrix using \emph{elimination templates}, which can
be done offline.

\subsection{Syzygy Modules}
A module over a ring is a generalization of the notion of a vector space over a field; however,
unlike vector spaces, most modules do not have bases. Consider the following example:

\begin{example}
The 
\end{example}

A submodule

\begin{example}
An ideal is a submodule
\end{example}

A \emph{free} module is a module that has a basis, and therefore behave much like vector spaces.

We will
\begin{definition}
Blah
\end{definition}
There for the syzygy module is the analogue of the null space in vector spaces as it is the kernel
of the map. The ambiguity of the choice of polynomials is therefore captured, and

\subsection{Saturation of an ideal}
Consider the following definition:
\begin{definition}
The \emph{saturation} of an ideal~$I\subset\K[x]$ \wrt{} the polynomial $f_s\in\K[x]$ is
defined as
\begin{equation}
    \sat(I,\, f_s) \coloneqq \{p\;|\;\exists N\geq 0,\, f_s^N p\in I  \}\;.
\end{equation}
\end{definition}
Consider the ideal $I=(x^2+xy,\, )\subset\C[x,\,y]$.

This is~\cite{larsson2017iccv}.
Comparison to \emph{Rabinowitsch trick}.

\subsection{Using symmetries}
Blahh

\subsection{The hidden variable trick}
In certain cases, you may have a linear relationship in one or more variables in your system.
We can then use the~\emph{hidden variable trick} to eliminate these variables,
leaving a simplified system. We illustrate this in the following example from computer vision.

\begin{example}\label{ex:hidden1}
Consider the relative pose problem with known rotation and calibrated cameras.
In this case the essential matrix is $\mat{E} = [\mat{t}]_\times$. The epipolar constraint
for a pair of corresponding points $\mat{x}_i\leftrightarrow\mat{x}_i'$,
is given by $\mat{x}'^T\mat{E}\mat{x}=0$. Since the translation vector~$\mat{t}\in\R^3$,
we have two degrees of freedom (due to the scale ambiguity), and therefore we require at least
two point correspondences in order to solve the problem. The minimal problem is therefore
\begin{equation}
\begin{aligned}
    \mat{x}_1'^T\mat{E}\mat{x}_1 &= 0, \\
    \mat{x}_2'^T\mat{E}\mat{x}_2 &= 0\;.
\end{aligned}
\end{equation}
Since these equations are linear in $\mat{t}$ they can be written as
\begin{equation}
    \mat{Mt} = \mat{0},
\end{equation}
where
\begin{equation}
\mat{M} = \begin{bmatrix}
    (\mat{x}_1\times\mat{x}_1')^\T \\
    (\mat{x}_2\times\mat{x}_2')^\T
\end{bmatrix}\;.
\end{equation}
The problem is therefore reduced to finding the null space of the $2\times 3$ matrix~$\mat{M}$.
In this case it can be done explicitly using elementary operations
\begin{equation}\label{eq:2pt_optimal}
    \mat{t} = (\mat{x}_1\times\mat{x}_1') \times (\mat{x}_2\times\mat{x}_2')\;.
\end{equation}
\end{example}

It is not always the case where we get the solution directly.
You might end up with a polynomial in a single variable, in which case a
simpler root finding method can be used, \eg{} by computing the eigenvalues
of the corresponding~\emph{companion matrix}, or---if the degree of the polynomial is
less than or equal to four---a closed-form solution can be obtained. We illustrate
the latter in the following example:

\begin{example}\label{ex:hidden2}
Consider the following system
\begin{equation}
\begin{aligned}
m_{11}xz + m_{12}x + m_{13}yz + m_{14}y + m_{15}z^2 + m_{16}z &= 0, \\
m_{22}xz + m_{22}x + m_{23}yz + m_{24}y + m_{25}z^2 + m_{26}z &= 0, \\
\end{aligned}
\end{equation}
where~$m_{ij}\in\C$ are generic coefficients.
By looking at a specific (non-degenerate) instance using Macaulay2
(or by using the~\emph{mixed cell method}) we see that there are in
general four solutions to this problem. One could use the action matrix method to construct an
elimination template; however, since the system is linear in $x$ and $y$ it can be rewritten as
\begin{equation}\label{eq:specialnullvector}
\mat{M}(z)
\begin{bmatrix}
x \\
y \\
1
\end{bmatrix}
= 0\;.
\end{equation}
where
\begin{equation}
\mat{M}(z)=
\begin{bmatrix}
    m_{11}z + m_{12} & m_{13}z + m_{14} & m_{15}z^2 + m_{16}z \\
    m_{22}z + m_{22} & m_{23}z + m_{24} & m_{25}z^2 + m_{26}z \\
    m_{32}z + m_{32} & m_{33}z + m_{34} & m_{35}z^2 + m_{36}z
\end{bmatrix}\;.
\end{equation}
In conclusion, we seek the one-dimensional nullspace of~$\mat{M}(z)$. Specifically, we want
the normalized null vector with last element equal to one. This means that $\mat{M}(z)$ must
be rank deficient and therefore $\det(\mat{M}(z))=0$. Computing this gives us a
quartic polynomial in~$z$
\begin{equation}
    \det(\mat{M}(z)) =
    \alpha_4 z^4 +
    \alpha_3 z^3 +
    \alpha_2 z^2 +
    \alpha_1 z +
    \alpha_0 = 0,
\end{equation}
where $\alpha_i\in\C$ only depend on~$m_{ij}$. Specifically, we have reduced the problem to
finding the roots of a quartic polynomial which has a closed-form solution, and is fast to compute.
We do not need to resolve to other more complex methods such as the action matrix method.
When the solutions to $z$ are found, we can extract $x$ and $y$ linearly, \eg{} by using SVD
on $\mat{M}(z^*)$, for a solution $z^*$, and normalize the corresponding null vector to have
last element equal to one.
\end{example}

In the following example, which is a modification of~\cref{ex:hidden1}, we show how that
the hidden variable trick can be used to reduce the number of unknowns, but still
apply the action matrix method to the resulting system.

\begin{example}
Consider the relative pose problem with known rotation but unknown and equal focal length and radial distortion.
Assume that the optical center and the distortion center coincide, and choose the local
coordinate system such that this point is at the origin. To correct for radial distortion we use
the one-parameter~\emph{division model}~\cite{fitzgibbon2001}, \ie{} the distorted
image points~$\mat{x}_d=(x,\,y,\,1)^\T$ are assumed to be mapped to their rectified counterpart~$\mat{x}_u$
using the following parametric relation
\begin{equation}
    \mat{x}_u = \phi(\mat{x}_d,\,\lambda) =
    \begin{bmatrix}
        x \\
        y \\
        1 + \lambda(x^2+y^2)
    \end{bmatrix}\;.
\end{equation}
Furthermore, the calibration matrix~$\mat{K}$ is
\begin{equation}
    \mat{K} = \begin{bmatrix}
    f\\ & f\\ && 1
    \end{bmatrix}, \qquad f > 0.
\end{equation}
In this case, the fundamental matrix is $\mat{F} = \mat{K}^{-1}[\mat{t}]_\times\mat{K}^{-1}$
and the epipolar constraint is~$\mat{x}_i'^\T\mat{Fx}_i = 0$ for two corresponding
points~$\mat{x}_i\leftrightarrow\mat{x}_i'$. Note that~$\mat{K}^{-1}$ contains $f^{-1}$ and
to make the equations polynomial we multiply each equation with~$f^2$. Furthermore, there
are four degrees of freedom (three translational parameters, the focal length and the radial
distortion coefficient, but the scale ambiguity is present); hence, the problem is minimal using four point correspondences.
More specifically, we want to solve
\begin{equation}
    f^2\cdot \phi(\mat{x}_i',\,\lambda)^\T \mat{F} \phi(\mat{x}_i,\,\lambda) = 0, \qquad \te{for }i=1,\ldots 4.
\end{equation}
Again, the equations are linear in~$\mat{t}$ and therefore we may consider the
system
\begin{equation}
\mat{M}(f,\,\lambda)\mat{t} = 0\;.
\end{equation}
Here the matrix~$\mat{M}$ is of size $4\times 3$, and must be rank deficient to allow a non-trivial
null vector. Therefore, all $3\times 3$ minors~$\mat{M}_i$ must vanish\footnote{A
$k\times k$ \emph{minor} of a matrix~$\mat{A}$ of size $m\times n$ is the determinant of a $k\times k$ matrix obtained by removing $m-k$ rows and $n-k$ columns.}, \ie{}
\begin{equation}
    \mat{M}_i(f,\,\lambda) = 0,  \qquad \te{for }i=1,\ldots 4.
\end{equation}
We now have a polynomial system with four equations and two unknowns (instead of four unknowns).
\end{example}

Sometimes it is beneficial (or even necessary) to involve saturation in the process.
In practice, this is often the case when you need to enforce a special structure to the
null vector, \cf{}~\cref{ex:hidden2}. In~\cref{eq:specialnullvector} we need the last
element to be equal to one; however, this condition is lost when only enforcing the minors
to vanish. It can happen that a one-dimensional family of false solutions is introduces corresponding
to null vectors where the last element is zero.
By using saturation, one may exclude such solutions. For practical scenarios where this is done
see~\cite{pritts2017,valtonenoernhag-etal-wacv-2021}.

\subsection{Eliminating variables}
Sometimes we are able to eliminate variables even if there are no linear relationships. This
has been used by several authors~\cite{kukelova-etal-cviu-2010,fraundorfer-etal-eccv-2010,jiang-etal-accv-2014,kukelova-etal-cvpr-2015,valtonenoernhag-springer-2021}.
Consider the following toy example
\begin{example}
Assume we have four unknowns $x$, $y$, $z$ and $w$, where $x^2+y^2=1$. We seek to solve
\begin{equation}
\mat{M}\mat{v} = 0,
\end{equation}
where $\mat{M}\in\C^{3\times 9}$ is a coefficient matrix and the monomial vector is
\setcounter{MaxMatrixCols}{20}
\begin{equation}
\mat{v} = \begin{bmatrix}
        xw & x &
        yw & y &
        z^2 & zw^2 & z &
         w & 1
    \end{bmatrix}^\T\;.
\end{equation}
Due to the constraint~$x^2+y^2=1$ we cannot use the hidden variable trick. But, we can still
eliminate $z$ by using Gauss--Jordan elimination. The corresponding system, after elimination, is on the form
\begin{equation}
\raisebox{-6pt}{ $\hat{\mat{M}} =$ }
\raisebox{-6pt}{\vast[}
\begingroup % keep the change local
\setlength\arraycolsep{2pt}
\begin{matrix}
%xw & x & yw & y & z^2 & zw^2 & zw & w & 1\\
z^2 & zw^2 & z & xw & x & yw & y & w & 1\\
    1 & & & \bullet & \bullet & \bullet & \bullet & \bullet & \bullet & \\
    & 1 & & \bullet & \bullet & \bullet & \bullet & \bullet & \bullet & \\
    & & 1 & \bullet & \bullet & \bullet & \bullet & \bullet & \bullet &
\end{matrix}
\endgroup
\raisebox{-6pt}{\vast]\;.}
\end{equation}
From the above system we see the following relations
\begin{equation}\label{paper04:eq:elim}
    \begin{aligned}
    z^2 + g_1(x,y,w)  &=  0,\\
    zw^2 + g_2(x,y,w)  &=  0,\\
    z + g_3(x,y,w)  &=  0,\\
    \end{aligned}
\end{equation}
where $g_i(x,y,w)$ are polynomials in the variables $x$, $y$ and~$w$.
Consequently, we must enforce the relations
\begin{equation}\label{paper04:eq:knowntilt1}
\begin{aligned}
    g_2(x,y,w) &= w^2 g_3(x,y,w), \\
    g_1(x,y,w) &= \left(g_3(x,y,w)\right)^2\;.
\end{aligned}
\end{equation}
Now, we have a reduced system with three unknowns and three equations given
by~\eqref{paper04:eq:knowntilt1} and the constraint $x^2+y^2=1$.
We will return to this problem during the lecture.

\iffalse
It turns out that~\eqref{paper04:eq:knowntilt1} are cubic and~\eqref{paper04:eq:knowntilt2} are quartic, and
by analyzing the dimension of the corresponding quotient ring, we find that
the system has six solutions in total (it can be verified that the original system has six
solutions as well). Using~\cite{larsson2018cvprb} an elimination template of size~$18\times 24$
was constructed.
\fi
\end{example}

\subsection{A clever elimination strategy}
This section is based on~\cite{kukelova-etal-2017-cvpr} and
we use the example of relative pose with unknown focal length. This is minimal with six
point correspondences.

Practical examples: \cite{ding-etal-2019-iccv,barath-kukelova-iccv-2019,valtonenoernhag-icpram-2019}

\subsection{Beyond Gröbner bases}
This section is based on~\cite{larsson2018cvpr}.
Practical examples~\cite{valtonenoernhag-springer-2021}

\subsection{Using PEP and other approaches for numerical stabilityi (or speed)}
Ding~\etal{} transformed their problem to a \emph{polynomial eigenvalue problem} (PEP)
to increase both numerical accuracy and speed~\cite{ding-etal-tpami-2020,ding-etal-cvpr-2020}.


\bibliographystyle{apalike}
{\small
\bibliography{alggeom_lecture_marcus}}

 \end{document}
